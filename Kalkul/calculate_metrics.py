# calculate_real_metrics.py
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import warnings

warnings.filterwarnings('ignore')

import tensorflow as tf

tf.get_logger().setLevel('ERROR')

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
import json
import sys

sys.path.append('.')
from hybrid_model import fuzzy_module, bayesian_module, train_lstm
from data_collector import preprocess


def calculate_real_metrics():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –º–µ—Ç—Ä–∏–∫ –≥—ñ–±—Ä–∏–¥–Ω–æ—ó –º–æ–¥–µ–ª—ñ."""
    print("=== –†–û–ó–†–ê–•–£–ù–û–ö –ú–ï–¢–†–ò–ö –ì–Ü–ë–†–ò–î–ù–û–á –ú–û–î–ï–õ–Ü ===")

    # 1. –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ë–Ü–õ–¨–®–ï –î–ê–ù–ò–•
    try:
        df = pd.read_csv("conn4_log_labeled.csv", low_memory=False, nrows=20000)
        print(f"‚úì –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(df)} —Ä—è–¥–∫—ñ–≤")
    except FileNotFoundError:
        print("‚úó –§–∞–π–ª conn4_log_labeled.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return None

    # 2. –ü–ï–†–ï–î–û–ë–†–û–ë–ö–ê
    try:
        processed = preprocess(df)
        print(f"‚úì –î–∞–Ω—ñ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {len(processed)} —Ä—è–¥–∫—ñ–≤, {len(processed.columns)} –æ–∑–Ω–∞–∫")
    except Exception as e:
        print(f"‚úó –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –¥–∞–Ω–∏—Ö: {e}")
        return None

    # 3. –ó–ù–ê–ô–¢–ò –ú–Ü–¢–ö–ò
    label_columns = ['anomaly', 'label', 'class', 'target', 'is_anomaly']
    found_label = next((col for col in label_columns if col in processed.columns), None)

    if not found_label:
        print("‚úó –°—Ç–æ–≤–ø–µ—Ü—å –∑ –º—ñ—Ç–∫–∞–º–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return None

    y = processed[found_label].values
    X = processed.drop(found_label, axis=1).values

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    unique_labels, counts = np.unique(y, return_counts=True)
    label_dist = dict(zip(unique_labels, counts))
    print(f"‚úì –†–æ–∑–ø–æ–¥—ñ–ª –º—ñ—Ç–æ–∫: {label_dist}")
    print(f"‚úì –î–∏—Å–±–∞–ª–∞–Ω—Å: {max(counts) / min(counts):.1f}:1")

    # 4. –ù–ê–í–ß–ò–¢–ò LSTM –ó –ü–û–ö–†–ê–©–ï–ù–û–Æ –ê–†–•–Ü–¢–ï–ö–¢–£–†–û–Æ
    print("üìä –ù–∞–≤—á–∞–Ω–Ω—è LSTM...")
    model, scaler = train_lstm(X, y, seq_length=10, epochs=50)

    # 5. –û–¢–†–ò–ú–ê–ù–ù–Ø –ü–†–û–ì–ù–û–ó–Ü–í
    seq_length = 10
    n_samples = len(X) - seq_length + 1
    print(f"üîç –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ –¥–ª—è {n_samples} –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π...")

    # –®–≤–∏–¥–∫–∏–π –±–∞—Ç—á-–ø—Ä–æ–≥–Ω–æ–∑
    all_sequences = np.array([X[i:i + seq_length] for i in range(n_samples)])

    if scaler:
        all_sequences = scaler.transform(all_sequences.reshape(-1, X.shape[1])).reshape(n_samples, seq_length,
                                                                                        X.shape[1])

    # LSTM –ø—Ä–æ–≥–Ω–æ–∑–∏
    lstm_model = tf.keras.models.load_model('lstm_model.keras', compile=False)
    lstm_scores = lstm_model.predict(all_sequences, verbose=0, batch_size=64).flatten()

    # Fuzzy —Ç–∞ Bayesian —Å–∫–æ—Ä–∏
    fuzzy_scores = []
    bayesian_score = bayesian_module({'Evidence': 1})

    for i in range(n_samples):
        seq = X[i:i + seq_length]
        features_mean = pd.Series(seq.mean(axis=0))
        fuzzy_scores.append(fuzzy_module(features_mean, [np.array([0.1, 0.5, 0.9])]))

    bayesian_scores = [bayesian_score] * n_samples

    # 6. –û–ë'–Ñ–î–ù–ê–ù–ù–Ø –ó –†–Ü–ó–ù–ò–ú–ò –í–ê–ì–ê–ú–ò –¢–ê –ü–û–†–û–ì–û–ú
    print("‚öñÔ∏è –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")

    # –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–Ü –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø:
    w1, w2, w3 = 0.1, 0.1, 0.8
    threshold = 0.3

    y_pred_scores = []
    y_pred_binary = []

    for i in range(n_samples):
        combined = (fuzzy_scores[i] * w1 +
                    bayesian_scores[i] * w2 +
                    lstm_scores[i] * w3)

        y_pred_scores.append(combined)
        y_pred_binary.append(1 if combined > threshold else 0)

    # –ü–û–°–¢-–û–ë–†–û–ë–ö–ê: –§–Ü–õ–¨–¢–†–ê–¶–Ü–Ø –ù–ò–ó–¨–ö–û–í–ü–ï–í–ù–ï–ù–ò–• –ê–ù–û–ú–ê–õ–Ü–ô
    print("üîß –ü–æ—Å—Ç-–æ–±—Ä–æ–±–∫–∞: —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –Ω–∏–∑—å–∫–æ–≤–ø–µ–≤–Ω–µ–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π...")

    confidence_threshold = 0.65  # ‚Üê –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£–ô–¢–ï –ó –¶–ò–ú
    filtered = 0

    for i in range(len(y_pred_binary)):
        if y_pred_binary[i] == 1 and y_pred_scores[i] < confidence_threshold:
            y_pred_binary[i] = 0
            filtered += 1

    print(f"   –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered} –Ω–∏–∑—å–∫–æ–≤–ø–µ–≤–Ω–µ–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π")

    # 7. –û–ë–ß–ò–°–õ–ï–ù–ù–Ø –ú–ï–¢–†–ò–ö
    y_true_trimmed = y[seq_length - 1:]

    accuracy = accuracy_score(y_true_trimmed, y_pred_binary)
    precision = precision_score(y_true_trimmed, y_pred_binary, zero_division=0)
    recall = recall_score(y_true_trimmed, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true_trimmed, y_pred_binary, zero_division=0)
    auc = roc_auc_score(y_true_trimmed, y_pred_scores)

    cm = confusion_matrix(y_true_trimmed, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()

    # 8. –†–ï–ó–£–õ–¨–¢–ê–¢–ò
    print("\n" + "=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–ò (–ø–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è):")
    print("=" * 60)
    print(f"Accuracy: {accuracy:.3f} ({accuracy * 100:.1f}%)")
    print(f"Precision: {precision:.3f} ({precision * 100:.1f}%)")
    print(f"Recall: {recall:.3f} ({recall * 100:.1f}%)")
    print(f"F1-Score: {f1:.3f} ({f1 * 100:.1f}%)")
    print(f"ROC-AUC: {auc:.3f}")
    print(f"\n–ú–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫:")
    print(f"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}")

    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
    total = tn + fp + fn + tp
    print(f"\n–ê–Ω–∞–ª—ñ–∑:")
    print(f"‚Ä¢ –ü—Ä–∞–≤–∏–ª—å–Ω–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤: {(tn + tp) / total * 100:.1f}%")
    print(f"‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π (FN): {fn / (fn + tp) * 100:.1f}%")
    print(f"‚Ä¢ –•–∏–±–Ω–∏—Ö —Ç—Ä–∏–≤–æ–≥ (FP): {fp / (tn + fp) * 100:.1f}%")

    return {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'roc_auc': float(auc),
        'confusion_matrix': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)}
    }


if __name__ == "__main__":
    calculate_real_metrics()